= Go Performance and Memory Optimization
Pradeep Roy

Main lessons while working on certain optimization changes in June 2025.

Input consisted of several files containing records from overlapping periods
of time. A software that was to consume these files had the limitation of
wanting timestamps in a globally sorted order. It was not sufficient for
records to be sorted in a single file. Therefore, we started writing a program
that would take input files with overlapping timestamps producing files that
encoded the time period in their names and ensured that all the records in the
input that were from the same time period were contained in it.  For instance,
a filename like `events-20250625100200-20250625100300.txt` would contain
records for the third minute of the 10th hour of 25-Jun-2025.

The program was required to deal with large amounts of data. For instance, 6
million records every minute, each record of size 500 bytes. This meant
sorting data for even a few minutes could cross the size of the available
system memory.

Input files consisted of newline separated JSON encoded arrays. CPU profiling
revealed that the program was spending a lot of time in decoding JSON records.
A crucial insight was that the program did not care about the fields in a
record except for the timestamp that was at the beginning. Therefore, decoding
the complete record using a JSON package was not necessary. Fields were comma
separated and so it was possible to quickly extract the timestamp by reading
till the first comma character.

Lesson 1: Decoding JSON is expensive.

The program's more than expected memory usage was related to a
`[]json.RawMessage` variable that temporarily stored every record in a loop
iterating through an input file.

Lesson 2: Go's runtime does not release memory to the OS immediately. Be
mindful of unintended memory allocation inside for loops.

The runtime environment's retaining memory manifested also through other
constructs in the program. It had to read all the records without needing to
understand any field apart from the timestamp. This was done by maintaining a
slice of struct that had a timestamp of type `int64` and the raw record of type
`[]byte`. The growth of the slice was left to the runtime using the `append()`
built-in function. Since the slice started with a length of 1 and could grow
till millions of entries, a lot of reallocation and copying was expected.
Memory held in old backing arrays could take time before being released to the
OS.

Lesson 3: Prefer preallocating to avoid repeated allocations and memory
retention.

Preallocation and careful inspection of loops to avoid new allocations brought
down the memory used by the program, which was now proportional to the input
size. But the performance did not increase even though there was parallelism.
The program's processing consisted of two phases. Phase 1 was about writing
records into various directories, each keeping a minute interval of records.
Phase 2 was about reading all records from files in each directory, sorting
them and writing to a single output file. The act of combining files for a
directory was independent of another. There were goroutines for doing this in
parallel. However, performance did not increase even if an attempt was made to
merge files from more directories in parallel.

Let us say phase 2 for 10 million records needs 6 GB of memory and takes 5
minutes to complete. These 5 minutes are broken down into these parts: (1) 70%
time for loading records into memory, (2) 1% time for sorting and (3) 29% time
for writing into a compressed format of size 600 MB. All of the three steps
have to be completed before it is possible to load the next batch of records
into memory. However, if we split the input into two equal parts, we can
create two output files each of size 300 MB in half the time while making
better use of CPU that was already available.

Lesson 4: When data is too large for memory break it into chunks and process
each independently. This not only reduces the use of memory but also increases
performance. We were already doing this by dividing the records into 1-minute
directories. The insight was that the same ideas could be pursued even
further.

